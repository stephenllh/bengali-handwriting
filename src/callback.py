import os
import numpy as np
from warnings import warn
import torch
from fastai.basic_train import LearnerCallback
from fastai.torchcore import add_metrics
from fastai.callback.core import Callback
from fastai.callback.tracker import TrackerCallback
from loss import MixUpLoss


class MetricIndex(Callback):
    def __init__(self, idx, average="macro"):
        super().__init__()
        self.idx = idx
        self.n_classes = 0
        self.average = average
        self.cm = None
        self.eps = 1e-9

    def on_epoch_begin(self, **kwargs):
        self.tp = 0
        self.fp = 0
        self.cm = None

    def on_batch_end(
        self, last_output: torch.Tensor, last_target: torch.Tensor, **kwargs
    ):
        last_output = last_output[self.idx]
        last_target = last_target[:, self.idx]
        preds = last_output.argmax(-1).view(-1).cpu()
        targs = last_target.long().cpu()

        if self.n_classes == 0:
            self.n_classes = last_output.shape[-1]
            self.x = torch.arange(0, self.n_classes)
        cm = ((preds == self.x[:, None]) & (targs == self.x[:, None, None])).sum(
            dim=2, dtype=torch.float32
        )
        if self.cm is None:
            self.cm = cm
        else:
            self.cm += cm

    def _weights(self, avg: str):
        if self.n_classes != 2 and avg == "binary":
            avg = self.average = "macro"
            warn(
                "average=`binary` was selected for a non binary case. \
                Value for average has now been set to `macro` instead."
            )

        if avg == "binary":
            if self.pos_label not in (0, 1):
                self.pos_label = 1
                warn("Invalid value for pos_label. It has now been set to 1.")
            if self.pos_label == 1:
                return torch.Tensor([0, 1])
            else:
                return torch.Tensor([1, 0])

        elif avg == "micro":
            return self.cm.sum(dim=0) / self.cm.sum()

        elif avg == "macro":
            return torch.ones((self.n_classes,)) / self.n_classes

        elif avg == "weighted":
            return self.cm.sum(dim=1) / self.cm.sum()

    def _recall(self):
        rec = torch.diag(self.cm) / (self.cm.sum(dim=1) + self.eps)
        if self.average is None:
            return rec
        else:
            if self.average == "micro":
                weights = self._weights(avg="weighted")
            else:
                weights = self._weights(avg=self.average)
            return (rec * weights).sum()

    def on_epoch_end(self, last_metrics, **kwargs):
        return add_metrics(last_metrics, self._recall())


class MetricTotal(Callback):
    def __init__(self):
        super().__init__()
        self.grapheme = MetricIndex(0)
        self.vowel = MetricIndex(1)
        self.consonant = MetricIndex(2)

    def on_epoch_begin(self, **kwargs):
        self.grapheme.on_epoch_begin(**kwargs)
        self.vowel.on_epoch_begin(**kwargs)
        self.consonant.on_epoch_begin(**kwargs)

    def on_batch_end(
        self, last_output: torch.Tensor, last_target: torch.Tensor, **kwargs
    ):
        self.grapheme.on_batch_end(last_output, last_target, **kwargs)
        self.vowel.on_batch_end(last_output, last_target, **kwargs)
        self.consonant.on_batch_end(last_output, last_target, **kwargs)

    def on_epoch_end(self, last_metrics, **kwargs):
        return add_metrics(
            last_metrics,
            0.5 * self.grapheme._recall()
            + 0.25 * self.vowel._recall()
            + 0.25 * self.consonant._recall(),
        )


class SaveModelCallback(TrackerCallback):
    """
    A `TrackerCallback` that saves the model when monitored quantity is best.
    Fixed the issue in fast.ai of saving gradients along with weights.
    So only weights are written, and files are ~4 times smaller""
    """

    def __init__(
        self,
        learn,
        monitor: str = "valid_loss",
        mode: str = "auto",
        every: str = "improvement",
        name: str = "bestmodel",
    ):
        super().__init__(learn, monitor=monitor, mode=mode)
        self.every, self.name = every, name
        self.learn = learn
        if self.every not in ["improvement", "epoch"]:
            warn(
                f'SaveModel every {self.every} is invalid, falling back to "improvement".'
            )
            self.every = "improvement"

    def jump_to_epoch(self, epoch: int) -> None:
        try:
            self.learn.load(f"{self.name}_{epoch-1}", purge=False)
            print(f"Loaded {self.name}_{epoch-1}")
        except Exception as ex:
            print(f"{ex}: Model {self.name}_{epoch-1} not found.")

    def on_epoch_end(self, epoch: int, **kwargs) -> None:
        "Compare the value monitored to its best score and maybe save the model."
        if self.every == "epoch":
            # self.learn.save(f'{self.name}_{epoch}')
            torch.save(self.learn.model.state_dict(), f"{self.name}_{epoch}.pth")
        else:  # every="improvement"
            current = self.get_monitor_value()
            if current is not None and self.operator(current, self.best):
                self.best = current
                torch.save(self.learn.model.state_dict(), f"{self.name}.pth")

    def on_train_end(self, **kwargs):
        "Load the best model."
        if self.every == "improvement" and os.path.isfile(f"{self.name}.pth"):
            # self.learn.load(f'{self.name}', purge=False)
            self.model.load_state_dict(torch.load(f"{self.name}.pth"))


class MixUpCallback(LearnerCallback):
    "Callback that creates the mixed-up input and target."

    def __init__(
        self,
        learn,
        alpha: float = 0.4,
        stack_x: bool = False,
        stack_y: bool = True,
    ):
        super().__init__(learn)
        self.alpha, self.stack_x, self.stack_y = alpha, stack_x, stack_y

    def on_train_begin(self, **kwargs):
        if self.stack_y:
            self.learn.loss_func = MixUpLoss(self.learn.loss_func)

    def on_batch_begin(self, last_input, last_target, train, **kwargs):
        "Applies mixup to `last_input` and `last_target` if `train`."
        if not train:
            return
        lambd = np.random.beta(self.alpha, self.alpha, last_target.size(0))
        lambd = np.concatenate([lambd[:, None], 1 - lambd[:, None]], 1).max(1)
        lambd = last_input.new(lambd)
        shuffle = torch.randperm(last_target.size(0)).to(last_input.device)
        x1, y1 = last_input[shuffle], last_target[shuffle]
        if self.stack_x:
            new_input = [last_input, last_input[shuffle], lambd]
        else:
            out_shape = [lambd.size(0)] + [1 for _ in range(len(x1.shape) - 1)]
            new_input = last_input * lambd.view(out_shape) + x1 * (1 - lambd).view(
                out_shape
            )
        if self.stack_y:
            new_target = torch.cat(
                [last_target.float(), y1.float(), lambd[:, None].float()], 1
            )
        else:
            if len(last_target.shape) == 2:
                lambd = lambd.unsqueeze(1).float()
            new_target = last_target.float() * lambd + y1.float() * (1 - lambd)
        return {"last_input": new_input, "last_target": new_target}

    def on_train_end(self, **kwargs):
        if self.stack_y:
            self.learn.loss_func = self.learn.loss_func.get_old()
